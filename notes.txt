The five bots (which operate independently but were trained using the same algorithms) were taught to play Dota 2 using a technique called reinforcement learning. This is a common training method that’s essentially trial-and-error at a huge scale. (It has its weaknesses, but it also produces incredible results, including AlphaGo.) Instead of coding the bots with the rules of Dota 2, they’re thrown into the game and left to figure things out for themselves. OpenAI’s engineers help this process along by rewarding them for completing certain tasks (like killing an opponent or winning a match) but nothing more than that.

This means the bots start out playing completely randomly, and over time, they learn to connect certain behaviors to rewards. As you might guess, this is an extremely inefficient way to learn. As a result, the bots have to play Dota 2 at an accelerated rate, cramming 180 years of training time into each day. As OpenAI’s CTO and co-founder Greg Brockman told The Verge earlier this year, if it takes a human between 12,000 and 20,000 hours of practice to master a certain skill, then the bots burn through “100 human lifetimes of experience every single day.”

to train their algorithms, OpenAI had to corral a massive amount of processing power — some 256 GPUs and 128,000 CPU cores. This is why experts often talk about the OpenAI Five as an engineering project as much as a research one: it’s an achievement just to get the system up and running, let alone beat the humans.

AI preferred “to win by 1 point with 90% certainty, than win by 50 points with a 51% certainty.” (This trait was also noticeable in AlphaGo’s game style.) It implies that OpenAI Five was used to grinding out steady but predictable victories. When the bots lost their lead, they were unable to make the more adventurous plays necessary to regain it.

This question of strategy is important not just for OpenAI, but for AI research more generally. The absence of long-term planning is often seen as a major flaw of reinforcement learning because AI created using this method often emphasize immediate payoffs rather than long-term rewards. This is because structuring a reward system that works over longer periods of time is difficult. How do you teach a bot to delay the use of a powerful spell until enemies are grouped together if you can’t predict when that will happen? Do you just give it small rewards for not using that spell? What if it decides never to use it as a result? And this is just one basic example. Dota 2 games generally last 30 to 45 minutes, and players have to constantly think through what action will lead to long-term success.

Unlike human gamers (or some other AI systems), they don’t actually look at the screen to play. Instead, they use Dota 2’s “bot API” to understand the game. This is a feed of 20,000 numbers that describes what’s going on in numerical form, incorporating information on everything from the location of each hero to their health to the cooldown on individual spells and attacks.

And it also helps those challenged by the machines. One of the most fascinating parts of the AlphaGo story was that although human champion Lee Sedol was beaten by an AI system, he, and the rest of the Go community, learned from it, too. AlphaGo’s play style upset centuries of accepted wisdom. Its moves are still being studied, and Lee went on a winning streak after his match against the machine.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
The inspiration for deep learning really comes from neuroscience. Look at the most successful deep learning networks. That’s convolutional neural networks, or CNNs, developed by Yann LeCun.

Let’s look at just one technological advance: the laser. It was invented about 50 years ago and it took up the whole room. To go from that room to the laser pointer I use when I give a lecture requires 50 years of commercialization of technology. It had to be advanced to the point where you shrink it down and buy it for five dollars. The same thing is going to happen to hyped technology like self-driving cars. It’s not expected to be ubiquitous next year or probably not 10 years. It may take 50, but the point, though, is that along the way there’ll be incremental advances that will make it more and more flexible, more safe and more compatible to the way we’ve organized our transportation grid. What’s wrong with the hype is that people have the timescale wrong. They’re expecting too much too soon, but in due time it will happen.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------	
“Billions of people inhabit the planet, each with their own individual goals and actions, but still capable of coming together through teams, organizations and societies in impressive displays of collective intelligence. This is a setting we call multi-agent learning: many individual agents must act independently, yet learn to interact and cooperate with other agents. This is an immensely difficult problem - because with co-adapting agents the world is constantly changing.”
Read more: https://www.smithsonianmag.com/smart-news/ai-learning-teamwork-dominating-video-games-180972322/#QetmkhTB37TdZLAP.99
Give the gift of Smithsonian magazine for only $12! http://bit.ly/1cGUiGv
Follow us: @SmithsonianMag on Twitter